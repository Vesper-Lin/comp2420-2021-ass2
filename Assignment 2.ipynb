{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'> COMP2420/COMP6420 - Introduction to Data Management,<br/> Analysis and Security</h1>\n",
    "\n",
    "<h1 align='center'> Assignment - 2</h1>\n",
    "\n",
    "-----\n",
    "\n",
    "|**Maximum Marks**         |**100**\n",
    "|--------------------------|--------\n",
    "|  **Weight**              |  **15% of the Total Course Grade**\n",
    "|  **Submission deadline** |  **8:00PM, Sunday, May 23th**\n",
    "|  **Submission mode**     |  **Electronic, Using GitLab**\n",
    "|  **Penalty**             |  **100% after the deadline**\n",
    "\n",
    "\n",
    "## Learning Outcomes\n",
    "The following learning outcomes apply to this piece:\n",
    "- **LO1** - Demonstrate a conceptual understanding of database systems and architecture, data models and declarative query languages\n",
    "- **LO2** - Define, query and manipulate a relational database\n",
    "- **LO3** - Demonstrate basic knowledge and understanding of descriptive and predictive data analysis methods, optimization and search, and knowledge representation.\n",
    "- **LO4** - Formulate and extract descriptive and predictive statistics from data\n",
    "- **LO5** - Analyse and interpret results from descriptive and predictive data analysis\n",
    "- **LO6** - Apply their knowledge to a given problem domain and articulate potential data analysis problems\n",
    "- **LO7** - Identify potential pitfalls, and social and ethical implications of data science\n",
    "- **LO8** - Explain key security concepts and the use of cryptographic techniques, digital signatures and PKI in security\n",
    "\n",
    "\n",
    "## Submission\n",
    "\n",
    "You need to submit the following items:\n",
    "- The notebook `Assignment-2-uXXXXXXX.ipynb` (where uXXXXXXX is your uid) \n",
    "- A completed `statement-of-originality.md`, found in the root of the forked gitlab repo.\n",
    "\n",
    "Submissions are performed by pushing to your forked GitLab assignment repository. For a refresher on forking and cloning repositories, please refer to `Lab 1`. Issues with your Git repo (with the exception of a CECS/ANU wide Gitlab failure) will not be considered as grounds for an extension. Any variation of this will result in a `zero mark`.\n",
    "\n",
    "***** \n",
    "\n",
    "### Notes:\n",
    "\n",
    "* It is strongly advised to read the whole assignment before attempting it and have at least a cursory glance at the dataset in order to gauge the requirements and understand what you need to do as a bigger picture.\n",
    "* Backup your assignment to your Gitlab repo often. \n",
    "* Extra reading and research will be required. Make sure you include all references in your Statement of Originality. If this does not occur, at best marks will be deduced. Otherwise, academic misconduct processes will be followed.\n",
    "* For answers requiring free form written text, use the designated cells denoted by `YOUR WRITTEN ANSWER HERE` -- double click on the cell to write inside them.\n",
    "* For all coding questions please write your code after the comment `YOUR CODE HERE`.\n",
    "* In the process of testing your code, you can insert more cells or use print statements for debugging, but when submitting your file remember to remove these cells and calls respectively. You are welcome to add additional cells to the final submission, provided they add value to the overall piece.\n",
    "* Your code answers will be marked on **correctness** and **readability** of your code, if your marker can't understand your code your marks may be deducted. \n",
    "* Your written answers will be marked on the **correctness**, **depth** and **clarity** of your written answers. If your marker cannot understand your answer, marks may be deducted\n",
    "* Before submitting, restart the kernel in Jupiter Lab and re-run all cells before submitting your code. This will ensure the namespace has not kept any old variables, as these won't come across in submission and your code will not run. Without this, you could lose a significant number of marks.\n",
    "\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter Your Student ID Below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "UID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "You have three (3) datasets to work with in this assignment, broken down as follows:\n",
    "\n",
    "- Questions 1 and 2 - Transcoding Dataset\n",
    "- Question 3 - SDSS\n",
    "- Question 4 - SuperStore Database\n",
    "\n",
    "Once again, the  dataset is a sizable dataset (roughly 8000 rows and 24 columns), so it is wise to consider your code in terms of complexity to ensure it doesn't take 30 minutes to run a single line.\n",
    "\n",
    "Further reading on the datasets can be found in the following locations:\n",
    "- [Transcoding Dataset.md](./data/cve/about.md)\n",
    "- [Northwind Database.md](./data/bikestores/about.md)\n",
    "- [SDSS Description](./data/SDSS_Description.txt)\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Imports\n",
    "# Every Lab import is here, you may need to uncomment additional items as necessary.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "#from scipy import stats\n",
    "#from sklearn.linear_model import LogisticRegression     # Logistic Regression\n",
    "#from sklearn.neighbors import KNeighborsClassifier      # k-Nearest Neighbours\n",
    "#from sklearn.preprocessing import LabelEncoder          # encooding variables\n",
    "#from sklearn.preprocessing import StandardScaler        # encooding variables\n",
    "#from sklearn.model_selection import train_test_split    # testing our models\n",
    "#from sklearn.preprocessing import OneHotEncoder         # nominal variable\n",
    "#from sklearn.metrics import confusion_matrix            # scoring\n",
    "#from sklearn.tree import DecisionTreeClassifier         # decision trees\n",
    "#from sklearn.tree import DecisionTreeRegressor          # decision trees\n",
    "#from sklearn import tree                                # decision trees\n",
    "#from sklearn.decomposition import PCA                   # PCA \n",
    "#from sklearn.cluster import KMeans                      # KMeans Clustering\n",
    "#from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional modules here as required\n",
    "# It is unlikely that you would need any additional modules, however we had added space here just in case you feel \n",
    "#     extras are required. Note that some justification as to WHY you are using them should be provided.\n",
    "#\n",
    "# Note that only modules in the standard Anaconda distribution are allowed. If you need to install it manually, it is not an accepted package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "## Q1: Clustering Videos for Transcoding <span style= 'float: right;'><b>[25 marks]</b></span>\n",
    "Building off the dataset you initially encountered in Assignment 1, the Transcoding dataset has made a return for the following question.\n",
    "\n",
    "The following question is designed to get you to load and process data and implement a clustering model for the given scenario below. You have been introduced to `KMeans` clustering in the lectures and labs, and this would therefore be the assumed clustering method, although you are welcome to supplement this with other clustering methods from the `sklearn` package as you desire.\n",
    "\n",
    "You will first be asked to import and pre-process the data ready to implement a clustering model. Then, you are on your own in the world of clustering. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Preprocessing <span style= 'float: right;'><b>[5 marks]</b></span>\n",
    "\n",
    "To start, bring in the data and get it ready for clustering. Your tasks are:\n",
    "\n",
    "1. Import the Data. The dataset is available in the location `data/transcoding_data.csv`.\n",
    "2. Check the dataset for any missing values and account for them.\n",
    "3. Prepare the data for a clustering task. You are welcome to use the data processing code that you wrote for the previous assignment.\n",
    "    - Drop irrelevant and redundant columns. Also drop the columns `frames`, `category`, `size`, `o_bitrate`, `codec`, and `o_codec`.\n",
    "    - Encode `o_resolution` as ordinal categorical variable with the order (176 x 144) $\\prec$ (320 x 240) $\\prec$ (480 x 360) $\\prec$ (640 x 480) $\\prec$ (1280 x 720) $\\prec$ (1920 x 1080).\n",
    "4. Provide descriptive statistics and display the first ten rows of the resulting dataset.\n",
    "\n",
    "Descriptive statistics generally consists of count, mean, standard deviation, min, max, and interquartile measures.\n",
    "The relation $a \\prec b$ states that the variable $a$ precedes $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 K-Means Clustering Implementation <span style= 'float: right;'><b>[10 marks]</b></span>\n",
    "\n",
    "Clustering helps visualise a dataset based on attributes considered important to the data scientist and/or reader.  Using the **Transcoding Dataset** above, implement a `K-Means clustering algorithm` to cluster the dataset of transcoded videos by using either all or a subset of the available features. Suppose you have used more than two features for your clustering; you are expected to reduce the dataset to either 2 or 3 dimensions.  After you have prepared your learning model, plot a **2D or 3D visualisation** showing the different clusters. \n",
    "\n",
    "It is up to you to decide how many clusters you would like to incorporate in your model. You are expected to **verbally and visually** justify your implementation, including the reasoning behind the choice of **the number of clusters** and **number of iterations** in your model. \n",
    "\n",
    "<span style='color:red;'><b>Note:</b> You are only allowed to use packages that are within the Anaconda distribution.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# YOUR RESPONSE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Analysing the Clusters <span style= 'float: right;'><b>[10 marks]</b></span>\n",
    "\n",
    "With your clustering model complete, analyse the outputs in preparation for showing the results to the procurement team. Create a DataFrame for each cluster's data and identify their main attributes - how do these clusters differ from each other. Provide a brief commentary of the clustering model based on the analysis along with a visualisation of the mean `umem` and `utime` across the different clusters.\n",
    "\n",
    "**Note:** Your analysis should include the mean and deviation of the continuous variables, mode of the categorical variables, and size of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# YOUR RESPONSE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "## Q2: KNN Classification for Video Transcoding <span style= 'float: right;'><b>[20 marks]</b></span>\n",
    "\n",
    "The rental company has decided that they want to use simple machine learning to allocate cost tags based on the transcoding score to the transcoded videos. The transcoding score can be obtained by taking the average of `umem` and `utime` and then normalizing it. The tags are as follows:\n",
    "\n",
    "\n",
    "|  **Classification**      |  **Requirements**        |\n",
    "|--------------------------|------------------------- |\n",
    "|  Cheap                   |  Transcoding Score is between 0.0 and 0.3   |\n",
    "|  Moderate                |  Transcoding Score is between 0.3 and 0.7 |\n",
    "|  Expensive               |  Transcoding Score is between 0.7 and 1.0 |\n",
    "\n",
    "\n",
    "**Note:** The `Classification` variable is an ordinal categorical variable whose ordering is Cheap $\\prec$ Moderate $\\prec$ Expensive where the relation $a \\prec b$ states that the variable $a$ precedes $b$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Your task is to implement a `K-Nearest Neighbours Classification Algorithm` that can predict the transcoding score of a video**. \n",
    "\n",
    "You are required to perform the following tasks:\n",
    "\n",
    "1. Use the same transcoding_data dataset as the previous question.\n",
    "2. Modify the dataset to create the `Transcoding Score` column by manually determining the prediction class for existing data (where the rating is within the classification system defined above)\n",
    "3. Implement an algorithm that can predict the `Transcoding Score` using the features present in the transcoding_data dataset.\n",
    "4. Perform independent testing of the model and provide statistical metrics outlining the performance of your model. Splitting the dataset into testing and training subsets will assist with this.\n",
    "\n",
    "You are welcome to use any features within the dataset, except the `umem` and `utime` of the transcoded video. Various attributes relating to the characteristics of the video and their respective transcoding settings in the tables can be helpful while making the algorithm. If required, you can also look to make new compound attributes that may be helpful in increasing the accuracy of your model.\n",
    "\n",
    "You are expected to **verbally and visually (wherever approriate)** justify all aspects of your answer, including the features used, the metrics provided and the validation system employed. Provide commentary on the strengths and potential pitfalls of the model.\n",
    "\n",
    "<span style='color:red;'><b>Note:</b> You are only allowed to use packages that are within the Anaconda distribution. This means packages such as Keras, Tensorflow etc are not available for use.</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Enter your Response and Predictors here (for marker simplicity)\n",
    "\n",
    "Response: \n",
    "Predictors: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# YOUR RESPONSE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "## Q3: Decision Trees for Digital Sky <span style= 'float: right;'><b>[20 marks]</b></span>\n",
    "\n",
    "The SDSS.csv data consists of 10,000 observations of space taken by the Sloan Digital Sky Survey, which offers public data of space observations. Every observation is described by 17 feature columns and 1 class column, which identifies it as either a star, galaxy or quasar.\n",
    "\n",
    "To ease your start with the data, you can read the feature descriptions in the [SDSS Description](./data/SDSS_Description.txt) file.\n",
    "\n",
    "**Your task is to implement a `Decision Tree Classification Algorithm` that can predict the `class` of an image**.\n",
    "\n",
    "You are required to perform the following tasks:\n",
    "\n",
    "1. Import the SDSS dataset and perform preprocessing as required.\n",
    "2. Create a new feature (or column) called `category` by converting the nominal categorical variable `class` into an ordinal categorical variable based on the table presented below:\n",
    "\n",
    "|  **class**      |  **category**        |\n",
    "|--------------------------|------------------------- |\n",
    "|  STAR                   |  0   |\n",
    "|  GALAXY                |  1 |\n",
    "|  QSO               |  2 |\n",
    "\n",
    "\n",
    "3. Implement an algorithm that can predict the `category` using the features present in the SDSS dataset.\n",
    "4. Perform independent testing of the model and provide statistical metrics outlining the performance of your model. Splitting the dataset into testing and training subsets will assist with this.\n",
    "5.  Plot the resulting Decision Tree produced by the Tree-Building algorithm.\n",
    "\n",
    "The QSO class refers to the quasi-stellar object, quasar. The order assigned to the `class` categorical variable is based on the brightness of the object, i.e. QSO is brighter than a GALAXY, and a GALAXY is brighter than a STAR.\n",
    "\n",
    "You are welcome to use any features within the dataset, except the `class` of the SDSS data. If required, you can also look to make new compound attributes that may help increase the accuracy of your model. You are expected to **verbally and visually** justify all aspects of your answer, including the features used, the metrics provided and the validation system employed. Provide commentary on the strengths and potential pitfalls of the model.\n",
    "\n",
    "<span style='color:red;'><b>Note:</b> You are only allowed to use packages that are within the Anaconda distribution. This means packages such as Keras, Tensorflow etc are not available for use.</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Enter your Response and Predictors here (for marker simplicity)\n",
    "\n",
    "Response: \n",
    "Predictors: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# YOUR RESPONSE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "## Q4: Serious SQL <span style= 'float: right;'><b>[20 marks]</b></span>\n",
    "Consider the following scenario.\n",
    "\n",
    "> You are applying for a job as a database developer for an unnamed wrestling company. Part of the job description includes creating an automation system for running SQL queries. During the hiring process, the interviewers want to ensure you understand the SQL language. They have provided a set of questions to be answered by you, and your responses will later be reviewed by them. They are unwilling to give you access to their real database (which is mysteriously missing), so they have provided an SQLite3 database and asked you to interact with it using Python. \n",
    "\n",
    "\n",
    "Based on the above scenario, you have been asked to answer a number of questions to test your skills. You will be using the Northwind database for this question. The database model is as follows:\n",
    "\n",
    "![Northwind_Database](./img/er_diagram.png)\n",
    "\n",
    "In the following questions, you will be asked to execute the SQL statement, and explain any reasoning as necessary. Data can be formatted as raw printed output or a Pandas DataFrame. Recall the use of the `fetchone` and `fetchall` functions on an sqlite cursor for retriving information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS YOUR CONNECTION BLOCK, DO NOT MODIFY THIS. \n",
    "# OTHERWISE, YOU WILL NOT BE ABLE TO READ THE DATABASE\n",
    "def create_connection():\n",
    "    \"\"\" create a database connection to a database that resides\n",
    "        in the memory\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(':memory:')\n",
    "        print(\"Connection established!\")\n",
    "        \n",
    "    except Error as e:\n",
    "        print(\"Error Connecting to Database\")\n",
    "        raise(e)\n",
    "    return conn\n",
    "\n",
    "northwind_sql = 'data/northwind.sql'\n",
    "conn = create_connection() \n",
    "cur = conn.cursor()\n",
    "qry = open(northwind_sql, 'r').read()\n",
    "cur.executescript(qry)\n",
    "conn.commit()\n",
    "# remember to close the connection when everything is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql(query):\n",
    "    # Select table and display\n",
    "    cur.execute(query)\n",
    "\n",
    "    # Fetches all the rows from the result of the query\n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    # Gets the column names for the table\n",
    "    colnames = [desc[0] for desc in cur.description]\n",
    "\n",
    "    # Converts into readable pandas dataframe\n",
    "    df_result = pd.DataFrame(rows, columns=colnames)\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: Customised Customers <span style= 'float: right;'><b>[3 marks]</b></span>\n",
    "\n",
    "Retrieve the details of all the customers whose `ContactTitle` is Owner or is located in the `Country` Mexico. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Customers With No Orders <span style= 'float: right;'><b>[3 marks]</b></span>\n",
    "There are some customers who have never actually placed an order. Show these customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3: Products & their Categories <span style= 'float: right;'><b>[3 marks]</b></span>\n",
    "\n",
    "Count the total quantity sold for each products. Retrieve the `ProductID`, `ProductName`, `CategoryName`, and the total quantity sold as `Total Sold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4: Products Above Average Price <span style= 'float: right;'><b>[3 marks]</b></span>\n",
    "\n",
    "Retrieve the `ProductName` and `UnitPrice` of all the products whose price is greater than the average price of all the products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5: Most Expensive Products <span style= 'float: right;'><b>[3 marks]</b></span>\n",
    "\n",
    "Get the `ProductName` and the `UnitPrice` of the top 10 most expensive products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6: Stocktaking Products by Category and Supplier <span style= 'float: right;'><b>[5 marks]</b></span>\n",
    "\n",
    "Get the total number of units that are in stock for each `ProductCategory` for each `Supplier Continent`. The resulting table should contain three columns: `ProductCategory`, `Supplier Continent`, and `UnitsInStock`.\n",
    "\n",
    "`Supplier Continent` can be obtained by mapping the values present in `Country` to their relevant continent. The table below contains the list of countries and the continent that they belong to.\n",
    "\n",
    "|  **Country**      |  **Supplier Continent**        |\n",
    "|--------------------------|------------------------- |\n",
    "|  UK, Spain, Sweden, Germany, Norway, Denmark, Netherlands, Finland, Italy, France                   |  Europe   |\n",
    "|  USA, Canada, Brazil                |  America |\n",
    "|  Australia, Japan, Singapore               |  Asia-Pacific |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "## Q5: Ethics and Security <span style= 'float: right;'><b>[15 marks]</b></span>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1: OkCupid Data Scrape <span style= 'float: right;'><b>[5 marks]</b></span>\n",
    "\n",
    "While Data Scientists and Data Engineers spend a lot of timing thinking about how to solve a problem, it is important to think about _why_ we solve a problem and what impacts it could have. For the following scenario, provide a written response to the questions.\n",
    "\n",
    "> In 2016, almost 70,000 Okcupid profiles had their data released onto the Open Science Framework. This place is an online community where people share raw data and collaborate with each other over data sets. Two Danish researchers, Emil Kirkegaard and Julius Daugbjerg-Bjerrekaer, scraped the data with a bot profile on Okcupid and released publicly identifiable information such as age, gender, sexual orientation, and personal responses to the survey questions the website asks when people sign up for a profile. More importantly, the two researchers didn’t feel their actions were explicitly or ethically wrong, because “Data is already public.” This huge data release raised eyebrows and forced questions about the ethics of releasing “already public” data. \n",
    "\n",
    "What does big data ethics have to say about already public data? What harms could arise from the outcomes of the two Danish researchers' actions?\n",
    "\n",
    "Provide examples in your response to the questions.\n",
    "\n",
    "**NOTE:** Marks will be awarded based on the brevity and clarity of the arguments and not on quantity. Do not exceed more than 300 words."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# YOUR RESPONSE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2: Digital Signature <span style= 'float: right;'><b>[8 marks]</b></span>\n",
    "\n",
    "Consider the following scenario:\n",
    "\n",
    "> Ray Technologies has outsourced some work to Lux and Kay, and needs them to jointly sign a contract, which it will then also sign. Since all of the parties involved are located in different parts of the world, and this is a frequently occurring scenario, Ray Technologies decides to come up with a method for doing this electronically. The contract has to be signed by both Lux and Kay, and then finally by Ray Technologies. We assume that the contract is transmitted electronically over public channels, so integrity and confidentiality have to be assured. Both Lux and Kay need to be assured that they are both signing the same contract and need to each have a copy of the contract signed by all three parties involved. The contract needs to be non-repudiable and the process has to be efficient.\n",
    "\n",
    "Describe a method that Ray Technologies can use for this purpose that uses cryptographic techniques and meets the above requirements."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# YOUR RESPONSE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3: Resilient to the future <span style= 'float: right;'><b>[2 marks]</b></span>\n",
    "\n",
    "As technology evolves, so does the need for secure cryptographic algorithms. With the introduction of quantum computing, the organisations around the world are preparing to migrate towards quantum resistant algorithms.\n",
    "\n",
    "Briefly explain why there is a need for quantum-resistant cryptographic algorithms."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# YOUR RESPONSE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
