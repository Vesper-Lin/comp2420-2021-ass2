{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'> COMP2420/COMP6420 - Introduction to Data Management,<br/> Analysis and Security</h1>\n",
    "\n",
    "<h1 align='center'> Assignment - 2</h1>\n",
    "\n",
    "-----\n",
    "\n",
    "|**Maximum Marks**         |**100**\n",
    "|--------------------------|--------\n",
    "|  **Weight**              |  **15% of the Total Course Grade**\n",
    "|  **Submission deadline** |  **8:00PM, Sunday, May 23th**\n",
    "|  **Submission mode**     |  **Electronic, Using GitLab**\n",
    "|  **Penalty**             |  **100% after the deadline**\n",
    "\n",
    "\n",
    "## Learning Outcomes\n",
    "The following learning outcomes apply to this piece:\n",
    "- **LO1** - Demonstrate a conceptual understanding of database systems and architecture, data models and declarative query languages\n",
    "- **LO2** - Define, query and manipulate a relational database\n",
    "- **LO3** - Demonstrate basic knowledge and understanding of descriptive and predictive data analysis methods, optimization and search, and knowledge representation.\n",
    "- **LO4** - Formulate and extract descriptive and predictive statistics from data\n",
    "- **LO5** - Analyse and interpret results from descriptive and predictive data analysis\n",
    "- **LO6** - Apply their knowledge to a given problem domain and articulate potential data analysis problems\n",
    "- **LO7** - Identify potential pitfalls, and social and ethical implications of data science\n",
    "- **LO8** - Explain key security concepts and the use of cryptographic techniques, digital signatures and PKI in security\n",
    "\n",
    "\n",
    "## Submission\n",
    "\n",
    "You need to submit the following items:\n",
    "- The notebook `Assignment-2.ipynb` \n",
    "- A completed `statement-of-originality.md`, found in the root of the forked gitlab repo.\n",
    "\n",
    "Submissions are performed by pushing to your forked GitLab assignment repository. For a refresher on forking and cloning repositories, please refer to `Lab 1`. Issues with your Git repo (with the exception of a CECS/ANU wide Gitlab failure) will not be considered as grounds for an extension. Any variation of this will result in a `zero mark`.\n",
    "\n",
    "***** \n",
    "\n",
    "### Notes:\n",
    "\n",
    "* It is strongly advised to read the whole assignment before attempting it and have at least a cursory glance at the dataset in order to gauge the requirements and understand what you need to do as a bigger picture.\n",
    "* Backup your assignment to your Gitlab repo often. \n",
    "* Extra reading and research will be required. Make sure you include all references in your Statement of Originality. If this does not occur, at best marks will be deduced. Otherwise, academic misconduct processes will be followed.\n",
    "* For answers requiring free form written text, use the designated cells denoted by `YOUR WRITTEN ANSWER HERE` -- double click on the cell to write inside them.\n",
    "* For all coding questions please write your code after the comment `YOUR CODE HERE`.\n",
    "* In the process of testing your code, you can insert more cells or use print statements for debugging, but when submitting your file remember to remove these cells and calls respectively. You are welcome to add additional cells to the final submission, provided they add value to the overall piece.\n",
    "* Your code answers will be marked on **correctness** and **readability** of your code, if your marker can't understand your code your marks may be deducted. \n",
    "* Your written answers will be marked on the **correctness**, **depth** and **clarity** of your written answers. If your marker cannot understand your answer, marks may be deducted\n",
    "* Before submitting, restart the kernel in Jupiter Lab and re-run all cells before submitting your code. This will ensure the namespace has not kept any old variables, as these won't come across in submission and your code will not run. Without this, you could lose a significant number of marks.\n",
    "\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter Your Student ID Below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "u6828533"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "You have three (3) datasets to work with in this assignment, broken down as follows:\n",
    "\n",
    "- Questions 1 and 2 - Transcoding Dataset\n",
    "- Question 3 - SDSS\n",
    "- Question 4 - SuperStore Database\n",
    "\n",
    "Once again, the  dataset is a sizable dataset (roughly 8000 rows and 24 columns), so it is wise to consider your code in terms of complexity to ensure it doesn't take 30 minutes to run a single line.\n",
    "\n",
    "Further reading on the datasets can be found in the following locations:\n",
    "- [Transcoding Dataset.md](./data/cve/about.md)\n",
    "- [Northwind Database.md](./data/bikestores/about.md)\n",
    "- [SDSS Description](./data/SDSS_Description.txt)\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Imports\n",
    "# Every Lab import is here, you may need to uncomment additional items as necessary.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LogisticRegression     # Logistic Regression\n",
    "from sklearn.neighbors import KNeighborsClassifier      # k-Nearest Neighbours\n",
    "from sklearn.preprocessing import LabelEncoder          # encooding variables\n",
    "from sklearn.preprocessing import StandardScaler        # encooding variables\n",
    "from sklearn.model_selection import train_test_split    # testing our models\n",
    "from sklearn.preprocessing import OneHotEncoder         # nominal variable\n",
    "from sklearn.metrics import confusion_matrix            # scoring\n",
    "from sklearn.tree import DecisionTreeClassifier         # decision trees\n",
    "from sklearn.tree import DecisionTreeRegressor          # decision trees\n",
    "from sklearn import tree                                # decision trees\n",
    "from sklearn.decomposition import PCA                   # PCA \n",
    "from sklearn.cluster import KMeans                      # KMeans Clustering\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional modules here as required\n",
    "# It is unlikely that you would need any additional modules, however we had added space here just in case you feel \n",
    "#     extras are required. Note that some justification as to WHY you are using them should be provided.\n",
    "#\n",
    "# Note that only modules in the standard Anaconda distribution are allowed. If you need to install it manually, it is not an accepted package.\n",
    "\n",
    "# JUST TO MAKE SURE SOME WARNINGS ARE IGNORED \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# visualization using seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# Silhouette Method For Optimal k\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# transcoding score normalizing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# find the optimal value of K in KNN\n",
    "from sklearn import metrics\n",
    "\n",
    "# build a text report showing the main classification metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# optimize hyper parameters of a DecisionTree model using Grid Search\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "## Q1: Clustering Videos for Transcoding <span style= 'float: right;'><b>[25 marks]</b></span>\n",
    "Building off the dataset you initially encountered in Assignment 1, the Transcoding dataset has made a return for the following question.\n",
    "\n",
    "The following question is designed to get you to load and process data and implement a clustering model for the given scenario below. You have been introduced to `KMeans` clustering in the lectures and labs, and this would therefore be the assumed clustering method, although you are welcome to supplement this with other clustering methods from the `sklearn` package as you desire.\n",
    "\n",
    "You will first be asked to import and pre-process the data ready to implement a clustering model. Then, you are on your own in the world of clustering. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Preprocessing <span style= 'float: right;'><b>[5 marks]</b></span>\n",
    "\n",
    "To start, bring in the data and get it ready for clustering. Your tasks are:\n",
    "\n",
    "1. Import the Data. The dataset is available in the location `data/transcoding_data.csv`.\n",
    "2. Check the dataset for any missing values and account for them.\n",
    "3. Prepare the data for a clustering task. You are welcome to use the data processing code that you wrote for the previous assignment.\n",
    "    - Drop irrelevant and redundant columns. Also drop the columns `frames`, `category`, `size`, `o_bitrate`, `codec`, and `o_codec`.\n",
    "    - Encode `o_resolution` as ordinal categorical variable with the order (176 x 144) $\\prec$ (320 x 240) $\\prec$ (480 x 360) $\\prec$ (640 x 480) $\\prec$ (1280 x 720) $\\prec$ (1920 x 1080).\n",
    "4. Provide descriptive statistics and display the first ten rows of the resulting dataset.\n",
    "\n",
    "Descriptive statistics generally consists of count, mean, standard deviation, min, max, and interquartile measures.\n",
    "The relation $a \\prec b$ states that the variable $a$ precedes $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)\n",
    "\n",
    "# 1. import data\n",
    "transcoding_data = pd.read_csv('./data/transcoding_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              False\n",
       "duration        False\n",
       "codec           False\n",
       "bitrate         False\n",
       "width           False\n",
       "height          False\n",
       "resolution      False\n",
       "framerate       False\n",
       "frames          False\n",
       "i               False\n",
       "p               False\n",
       "b               False\n",
       "size            False\n",
       "category        False\n",
       "url             False\n",
       "o_codec         False\n",
       "o_bitrate       False\n",
       "o_framerate     False\n",
       "o_width         False\n",
       "o_height        False\n",
       "o_resolution    False\n",
       "umem            False\n",
       "utime           False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. check for any missing values \n",
    "# the result below shows there are no missing values in the dataset\n",
    "transcoding_data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. prepare for clustering\n",
    "# drop columns\n",
    "transcoding_data.drop([\n",
    "    # irrelevant columns\n",
    "    'id', 'url', \n",
    "    # redundant columns\n",
    "    'width', 'height', 'o_width', 'o_height',\n",
    "    # designated columns\n",
    "    'frames', 'category', 'size', 'o_bitrate', 'codec', 'o_codec'\n",
    "], axis = 1, inplace=True)\n",
    "\n",
    "# archive data\n",
    "transcoding_data_copy = transcoding_data.copy()\n",
    "\n",
    "def convert_o_resolution(object):\n",
    "    if object == '176x144':\n",
    "        return '1'\n",
    "    if object == '320x240':\n",
    "        return '2'\n",
    "    if object == '480x360':\n",
    "        return '3'\n",
    "    if object == '640x480':\n",
    "        return '4'\n",
    "    if object == '1280x720':\n",
    "        return '5'\n",
    "    if object == '1920x1080':\n",
    "        return '6'\n",
    "\n",
    "# encode o_resolution as ordinal categorical variable    \n",
    "transcoding_data['o_resolution'] = transcoding_data['o_resolution'].apply(convert_o_resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>bitrate</th>\n",
       "      <th>framerate</th>\n",
       "      <th>i</th>\n",
       "      <th>p</th>\n",
       "      <th>b</th>\n",
       "      <th>o_framerate</th>\n",
       "      <th>umem</th>\n",
       "      <th>utime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4890</td>\n",
       "      <td>4890</td>\n",
       "      <td>4890</td>\n",
       "      <td>4890</td>\n",
       "      <td>4890</td>\n",
       "      <td>4890</td>\n",
       "      <td>4890</td>\n",
       "      <td>4890</td>\n",
       "      <td>4890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1277</td>\n",
       "      <td>788</td>\n",
       "      <td>23</td>\n",
       "      <td>399</td>\n",
       "      <td>28510</td>\n",
       "      <td>116</td>\n",
       "      <td>21</td>\n",
       "      <td>197016</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1554</td>\n",
       "      <td>1166</td>\n",
       "      <td>8</td>\n",
       "      <td>497</td>\n",
       "      <td>36594</td>\n",
       "      <td>158</td>\n",
       "      <td>7</td>\n",
       "      <td>195384</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>3970</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>405</td>\n",
       "      <td>91</td>\n",
       "      <td>12</td>\n",
       "      <td>116</td>\n",
       "      <td>7731</td>\n",
       "      <td>29</td>\n",
       "      <td>15</td>\n",
       "      <td>63693</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>812</td>\n",
       "      <td>347</td>\n",
       "      <td>25</td>\n",
       "      <td>247</td>\n",
       "      <td>17230</td>\n",
       "      <td>65</td>\n",
       "      <td>24</td>\n",
       "      <td>130939</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1545</td>\n",
       "      <td>837</td>\n",
       "      <td>30</td>\n",
       "      <td>495</td>\n",
       "      <td>35080</td>\n",
       "      <td>143</td>\n",
       "      <td>25</td>\n",
       "      <td>260130</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>25845</td>\n",
       "      <td>14502</td>\n",
       "      <td>49</td>\n",
       "      <td>7032</td>\n",
       "      <td>530243</td>\n",
       "      <td>2329</td>\n",
       "      <td>30</td>\n",
       "      <td>1315594</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       duration  bitrate  framerate     i       p     b  o_framerate     umem  \\\n",
       "count      4890     4890       4890  4890    4890  4890         4890     4890   \n",
       "mean       1277      788         23   399   28510   116           21   197016   \n",
       "std        1554     1166          8   497   36594   158            7   195384   \n",
       "min           1        9          0     0      12     0           12     3970   \n",
       "25%         405       91         12   116    7731    29           15    63693   \n",
       "50%         812      347         25   247   17230    65           24   130939   \n",
       "75%        1545      837         30   495   35080   143           25   260130   \n",
       "max       25845    14502         49  7032  530243  2329           30  1315594   \n",
       "\n",
       "       utime  \n",
       "count   4890  \n",
       "mean     162  \n",
       "std       72  \n",
       "min        1  \n",
       "25%      107  \n",
       "50%      155  \n",
       "75%      206  \n",
       "max      614  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. descriptive statistics\n",
    "transcoding_data.describe().round(0).applymap(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>bitrate</th>\n",
       "      <th>resolution</th>\n",
       "      <th>framerate</th>\n",
       "      <th>i</th>\n",
       "      <th>p</th>\n",
       "      <th>b</th>\n",
       "      <th>o_framerate</th>\n",
       "      <th>o_resolution</th>\n",
       "      <th>umem</th>\n",
       "      <th>utime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>267</td>\n",
       "      <td>373</td>\n",
       "      <td>568x320</td>\n",
       "      <td>29.97</td>\n",
       "      <td>102</td>\n",
       "      <td>7858</td>\n",
       "      <td>40</td>\n",
       "      <td>15.00</td>\n",
       "      <td>3</td>\n",
       "      <td>172247.935215</td>\n",
       "      <td>166.415727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>267</td>\n",
       "      <td>324</td>\n",
       "      <td>400x226</td>\n",
       "      <td>29.97</td>\n",
       "      <td>130</td>\n",
       "      <td>7816</td>\n",
       "      <td>54</td>\n",
       "      <td>12.00</td>\n",
       "      <td>2</td>\n",
       "      <td>115355.838797</td>\n",
       "      <td>51.035704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>267</td>\n",
       "      <td>85</td>\n",
       "      <td>176x144</td>\n",
       "      <td>12.00</td>\n",
       "      <td>43</td>\n",
       "      <td>3146</td>\n",
       "      <td>14</td>\n",
       "      <td>24.00</td>\n",
       "      <td>4</td>\n",
       "      <td>37827.340566</td>\n",
       "      <td>173.629478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>1261</td>\n",
       "      <td>640x480</td>\n",
       "      <td>24.00</td>\n",
       "      <td>58</td>\n",
       "      <td>714</td>\n",
       "      <td>28</td>\n",
       "      <td>24.00</td>\n",
       "      <td>4</td>\n",
       "      <td>110002.908561</td>\n",
       "      <td>150.693204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>370</td>\n",
       "      <td>320x240</td>\n",
       "      <td>24.00</td>\n",
       "      <td>61</td>\n",
       "      <td>674</td>\n",
       "      <td>8</td>\n",
       "      <td>29.97</td>\n",
       "      <td>6</td>\n",
       "      <td>77903.861755</td>\n",
       "      <td>278.413210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>333</td>\n",
       "      <td>727</td>\n",
       "      <td>384x288</td>\n",
       "      <td>25.00</td>\n",
       "      <td>139</td>\n",
       "      <td>8157</td>\n",
       "      <td>27</td>\n",
       "      <td>15.00</td>\n",
       "      <td>3</td>\n",
       "      <td>90781.115843</td>\n",
       "      <td>130.548432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>333</td>\n",
       "      <td>306</td>\n",
       "      <td>320x240</td>\n",
       "      <td>25.00</td>\n",
       "      <td>163</td>\n",
       "      <td>8119</td>\n",
       "      <td>42</td>\n",
       "      <td>24.00</td>\n",
       "      <td>5</td>\n",
       "      <td>54266.504159</td>\n",
       "      <td>184.049056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>333</td>\n",
       "      <td>91</td>\n",
       "      <td>176x144</td>\n",
       "      <td>12.00</td>\n",
       "      <td>57</td>\n",
       "      <td>3920</td>\n",
       "      <td>18</td>\n",
       "      <td>15.00</td>\n",
       "      <td>3</td>\n",
       "      <td>18842.877160</td>\n",
       "      <td>59.506751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>67</td>\n",
       "      <td>633</td>\n",
       "      <td>854x480</td>\n",
       "      <td>29.92</td>\n",
       "      <td>26</td>\n",
       "      <td>1974</td>\n",
       "      <td>4</td>\n",
       "      <td>29.97</td>\n",
       "      <td>6</td>\n",
       "      <td>236610.816723</td>\n",
       "      <td>235.736431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>67</td>\n",
       "      <td>302</td>\n",
       "      <td>400x224</td>\n",
       "      <td>29.92</td>\n",
       "      <td>52</td>\n",
       "      <td>1922</td>\n",
       "      <td>29</td>\n",
       "      <td>24.00</td>\n",
       "      <td>6</td>\n",
       "      <td>104387.945170</td>\n",
       "      <td>220.047137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration  bitrate resolution  framerate    i     p   b  o_framerate  \\\n",
       "0       267      373    568x320      29.97  102  7858  40        15.00   \n",
       "1       267      324    400x226      29.97  130  7816  54        12.00   \n",
       "2       267       85    176x144      12.00   43  3146  14        24.00   \n",
       "3        31     1261    640x480      24.00   58   714  28        24.00   \n",
       "4        31      370    320x240      24.00   61   674   8        29.97   \n",
       "5       333      727    384x288      25.00  139  8157  27        15.00   \n",
       "6       333      306    320x240      25.00  163  8119  42        24.00   \n",
       "7       333       91    176x144      12.00   57  3920  18        15.00   \n",
       "8        67      633    854x480      29.92   26  1974   4        29.97   \n",
       "9        67      302    400x224      29.92   52  1922  29        24.00   \n",
       "\n",
       "  o_resolution           umem       utime  \n",
       "0            3  172247.935215  166.415727  \n",
       "1            2  115355.838797   51.035704  \n",
       "2            4   37827.340566  173.629478  \n",
       "3            4  110002.908561  150.693204  \n",
       "4            6   77903.861755  278.413210  \n",
       "5            3   90781.115843  130.548432  \n",
       "6            5   54266.504159  184.049056  \n",
       "7            3   18842.877160   59.506751  \n",
       "8            6  236610.816723  235.736431  \n",
       "9            6  104387.945170  220.047137  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. the first ten rows\n",
    "transcoding_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 K-Means Clustering Implementation <span style= 'float: right;'><b>[10 marks]</b></span>\n",
    "\n",
    "Clustering helps visualise a dataset based on attributes considered important to the data scientist and/or reader.  Using the **Transcoding Dataset** above, implement a `K-Means clustering algorithm` to cluster the dataset of transcoded videos by using either all or a subset of the available features. Suppose you have used more than two features for your clustering; you are expected to reduce the dataset to either 2 or 3 dimensions.  After you have prepared your learning model, plot a **2D or 3D visualisation** showing the different clusters. \n",
    "\n",
    "It is up to you to decide how many clusters you would like to incorporate in your model. You are expected to **verbally and visually** justify your implementation, including the reasoning behind the choice of **the number of clusters** and **number of iterations** in your model. \n",
    "\n",
    "<span style='color:red;'><b>Note:</b> You are only allowed to use packages that are within the Anaconda distribution.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)\n",
    "\n",
    "# encode\n",
    "le = LabelEncoder()\n",
    "transcoding_data['resolution'] = le.fit_transform(transcoding_data['resolution'])\n",
    "\n",
    "# scale \n",
    "ss = StandardScaler()\n",
    "ss.fit(transcoding_data)\n",
    "transcoding_data_scaled = ss.transform(transcoding_data)\n",
    "\n",
    "# decompose\n",
    "pca = PCA(n_components=2)\n",
    "transcoding_data_reduced = pca.fit_transform(transcoding_data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Elbow Method For Optimal k\n",
    "\n",
    "distortions = []\n",
    "K = range(1,10)\n",
    "\n",
    "for k in K:\n",
    "    kmc = KMeans(n_clusters=k)\n",
    "    kmc.fit(transcoding_data_reduced)\n",
    "    distortions.append(kmc.inertia_)\n",
    "    \n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The Elbow Method returns an ambiguous plot for determining the elbow k. We could choose k to be either 3 or 4. Thus, I choose to use the Silhouette Method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Method For Optimal k\n",
    "\n",
    "sil = []\n",
    "kmax = 10\n",
    "\n",
    "# dissimilarity would not be defined for a single cluster, thus, minimum number of clusters should be 2\n",
    "for k in range(2, kmax+1):\n",
    "  kmc = KMeans(n_clusters = k).fit(transcoding_data_reduced)\n",
    "  labels = kmc.labels_\n",
    "  sil.append(silhouette_score(transcoding_data_reduced, labels, metric = 'euclidean'))\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(K, sil, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The Silhouette Score reaches its global maximum at the optimal k (Mahendru, 2019). The Silhouette Method returns a plot that indicates a clear peak at k = 3. Hence, k = 3 is optimal.\n",
    "\n",
    "Finally, the data can be optimally clustered into 3 clusters as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total rows in the transcoding dataset is:\", len(transcoding_data_reduced))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Since k=3 and there are 4890 rows in the dataset, I set the maximum number of iterations to be 1630.\n",
    "\n",
    "Because 1630 * 3 = 4890, the algorithm is able to converge in a finite number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 clusters\n",
    "# random seed set so that analysis is consistent below\n",
    "kmc = KMeans(n_clusters=3, random_state=0, max_iter=1630)\n",
    "kmc_model = kmc.fit(transcoding_data_reduced)\n",
    "\n",
    "# plotting \n",
    "markers = ['*', 'x', 's', 'D', 'P', 'X']\n",
    "colors=[\"tomato\",\"royalblue\",\"gold\",\"rebeccapurple\",\"limegreen\",\"rosybrown\"]\n",
    "plt.figure(figsize=(14, 10))\n",
    "for i in range(np.max(kmc_model.labels_)+1):\n",
    "    plt.scatter(transcoding_data_reduced[kmc_model.labels_==i][:,0], transcoding_data_reduced[kmc_model.labels_==i][:,1], label=i, c=colors[i], marker=markers[i], alpha=0.5)\n",
    "plt.scatter(kmc_model.cluster_centers_[:,0], kmc_model.cluster_centers_[:,1], label='Cluster Centers', c='black', marker='P', s=200)\n",
    "plt.title(\"K-Means Clustering of Transcoding Data\",size=20)\n",
    "plt.xlabel(\"Principle Component 1\", size=16)\n",
    "plt.ylabel(\"Principle Component 2\", size=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# YOUR RESPONSE HERE\n",
    "\n",
    "1. I encode the Object variable 'resolution' to ordinal categorical variable by LabelEncoder, so that resolution can be considered when clustering.\n",
    "\n",
    "2. I scale the data by StandardScaler, so that the range of a predictor doesn't affect cluster distribution.\n",
    "\n",
    "3. I reduce the dimensionality of the data to 2 dimensions by Principle Components Analysis (PCA), so that I can look at the data from the best angle. Even though there are 11 variables originally, PCA decomposes all of the variables into 2 dimensions that maximise the variance between them.\n",
    "\n",
    "4. I use the Elbow Method and the Silhouette Method to determine the optimal number of clusters for fitting the K-Means model. The Elbow Method returns an ambiguous result so I turn to the Silhouette Method, and according to (Mahendru, 2019), the result's peak value k = 3 is optimal.\n",
    "\n",
    "5. I set the maximum number of iterations to be 1630 according to the value of k and the total number of rows in the dataset, hence the model converges in the iterations.\n",
    "\n",
    "6. I cluster the transcoding data optimally into 3 clusters by K-Means clustering, and visualize the result on a 2D plot. The plot represents 3 clusters in 2 dimensions of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Analysing the Clusters <span style= 'float: right;'><b>[10 marks]</b></span>\n",
    "\n",
    "With your clustering model complete, analyse the outputs in preparation for showing the results to the procurement team. Create a DataFrame for each cluster's data and identify their main attributes - how do these clusters differ from each other. Provide a brief commentary of the clustering model based on the analysis along with a visualisation of the mean `umem` and `utime` across the different clusters.\n",
    "\n",
    "**Note:** Your analysis should include the mean and deviation of the continuous variables, mode of the categorical variables, and size of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)\n",
    "\n",
    "# convert types\n",
    "transcoding_data_copy['resolution'].astype(str)\n",
    "transcoding_data_copy['o_resolution'].astype(str)\n",
    "\n",
    "# split data and analyse\n",
    "k = np.max(kmc_model.labels_)+1\n",
    "df_clusters = [transcoding_data_copy[kmc_model.labels_==i] for i in range(k)]\n",
    "\n",
    "stat_dict = { \n",
    "    # size of each cluster\n",
    "    'Cluster' : list(range(k)),\n",
    "    'Size' :    [len(df_clusters[i]) for i in range(k)],\n",
    "    # mode of the categorical variables\n",
    "    'Mode Resolution':                  [df_clusters[i]['resolution'].value_counts().idxmax() for i in range(k)],\n",
    "    'Mode Output_Resolution':           [df_clusters[i]['o_resolution'].value_counts().idxmax() for i in range(k)],\n",
    "    # mean and deviation of the continuous variables\n",
    "    'Mean Duration' :                   [round(df_clusters[i]['duration'].mean(), 0).astype(int) for i in range(k)],\n",
    "    'Mean Bitrate' :                    [round(df_clusters[i]['bitrate'].mean(), 0).astype(int) for i in range(k)],\n",
    "    'Mean Framerate' :                  [round(df_clusters[i]['framerate'].mean(), 0).astype(int) for i in range(k)],\n",
    "    'Mean Memory' :                     [round(df_clusters[i]['umem'].mean(), 0).astype(int) for i in range(k)],\n",
    "    'Mean Time' :                       [round(df_clusters[i]['utime'].mean(), 0).astype(int) for i in range(k)],\n",
    "    'Std Dev Duration' :                [round(df_clusters[i]['duration'].std(), 0).astype(int) for i in range(k)],\n",
    "    'Std Dev Bitrate' :                 [round(df_clusters[i]['bitrate'].std(), 0).astype(int) for i in range(k)],\n",
    "    'Std Dev Framerate' :               [round(df_clusters[i]['framerate'].std(), 0).astype(int) for i in range(k)],\n",
    "    'Std Dev Memory' :                  [round(df_clusters[i]['umem'].std(), 0).astype(int) for i in range(k)],\n",
    "    'Std Dev Time' :                    [round(df_clusters[i]['utime'].std(), 0).astype(int) for i in range(k)]\n",
    "}\n",
    "df_cluster_stats = pd.DataFrame(stat_dict)\n",
    "df_cluster_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), gridspec_kw={\"width_ratios\": [1, 1]}) \n",
    "\n",
    "sns.barplot(data=df_cluster_stats, x='Cluster', y='Mean Memory', palette='rainbow', ax=axes[0])\n",
    "sns.barplot(data=df_cluster_stats, x='Cluster', y='Mean Time', palette='rainbow', ax=axes[1])\n",
    "\n",
    "axes[0].set_title('Mean Memory for Diffrent Clusters')\n",
    "axes[1].set_title('Mean Time for Diffrent Clusters')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# YOUR RESPONSE HERE\n",
    "\n",
    "From the df_cluster_stats DataFrame and the plots above, we see that:\n",
    "\n",
    "@Cluster 0: \n",
    "~continuous variables: low duration, low bitrate, low framerate, low memory, low time | low standard deviation in bitrate, memory, time\n",
    "~categorical variables: lowest resolution and lowest output resolution\n",
    "~size of cluster: the largest cluster\n",
    "           \n",
    "@Cluster 1: \n",
    "~continuous variables: high duration, moderate bitrate, moderate framerate, moderate memory, high time | moderate standard deviation\n",
    "~categorical variables: low resolution and low output resolution\n",
    "~size of cluster: the smallest cluster\n",
    "           \n",
    "@Cluster 2: \n",
    "~continuous variables: moderate duration, high bitrate, high framerate, high memory, high time | low standard deviation in duration, framerate\n",
    "~categorical variables: highest resolution and highest output resolution\n",
    "~size of cluster: the second largest cluster\n",
    "           \n",
    "           \n",
    "In sum, we conclude that:\n",
    "\n",
    "@Cluster 0 is characterised by low continuous variables (including duration, bitrate, framerate, memory, and time), low resolution and output resolution, and is the largest cluster.\n",
    "\n",
    "@Cluster 1 is characterised by high duration, high time, low resolution and output resolution, and is the smallest cluster.\n",
    "\n",
    "@Cluster 2 is characterised by high bitrate, high framerate, high memory, high time, high resolution and output resolution, and is the second largest cluster.\n",
    "\n",
    "If we focus on the mean values of Total Codec Allocated Memory (umem) and Total Process Time (utime) for Transcoding, @Cluster 0 is the collection which uses least memory and time becasue of the lowest continiuous variables and lowest categorical variables, while @Cluster 2 uses most time and memory because of the high continiuous variables and the highest categorical variables. @Cluster 1 needs plenty of process time because of the highest mean duration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "## Q2: KNN Classification for Video Transcoding <span style= 'float: right;'><b>[20 marks]</b></span>\n",
    "\n",
    "The rental company has decided that they want to use simple machine learning to allocate cost tags based on the transcoding score to the transcoded videos. The transcoding score can be obtained by taking the average of `umem` and `utime` and then normalizing it. The tags are as follows:\n",
    "\n",
    "\n",
    "|  **Classification**      |  **Requirements**        |\n",
    "|--------------------------|------------------------- |\n",
    "|  Cheap                   |  Transcoding Score is between 0.0 and 0.3   |\n",
    "|  Moderate                |  Transcoding Score is between 0.3 and 0.7 |\n",
    "|  Expensive               |  Transcoding Score is between 0.7 and 1.0 |\n",
    "\n",
    "\n",
    "**Note:** The `Classification` variable is an ordinal categorical variable whose ordering is Cheap $\\prec$ Moderate $\\prec$ Expensive where the relation $a \\prec b$ states that the variable $a$ precedes $b$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Your task is to implement a `K-Nearest Neighbours Classification Algorithm` that can predict the transcoding score of a video**. \n",
    "\n",
    "You are required to perform the following tasks:\n",
    "\n",
    "1. Use the same transcoding_data dataset as the previous question.\n",
    "2. Modify the dataset to create the `Transcoding Score` column by manually determining the prediction class for existing data (where the rating is within the classification system defined above)\n",
    "3. Implement an algorithm that can predict the `Transcoding Score` using the features present in the transcoding_data dataset.\n",
    "4. Perform independent testing of the model and provide statistical metrics outlining the performance of your model. Splitting the dataset into testing and training subsets will assist with this.\n",
    "\n",
    "You are welcome to use any features within the dataset, except the `umem` and `utime` of the transcoded video. Various attributes relating to the characteristics of the video and their respective transcoding settings in the tables can be helpful while making the algorithm. If required, you can also look to make new compound attributes that may be helpful in increasing the accuracy of your model.\n",
    "\n",
    "You are expected to **verbally and visually (wherever approriate)** justify all aspects of your answer, including the features used, the metrics provided and the validation system employed. Provide commentary on the strengths and potential pitfalls of the model.\n",
    "\n",
    "<span style='color:red;'><b>Note:</b> You are only allowed to use packages that are within the Anaconda distribution. This means packages such as Keras, Tensorflow etc are not available for use.</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)\n",
    "\n",
    "# inherit the transcoding dataset\n",
    "knn_data = transcoding_data_copy\n",
    "\n",
    "# encode the categorical variables\n",
    "le = LabelEncoder()\n",
    "knn_data['resolution'] = le.fit_transform(knn_data['resolution'])\n",
    "knn_data['o_resolution'] = le.fit_transform(knn_data['o_resolution'])\n",
    "\n",
    "# calculate transcoding score by taking the average of umem and utime\n",
    "cols = knn_data.loc[: , 'umem' : 'utime']\n",
    "knn_data['score'] = cols.mean(axis=1)\n",
    "\n",
    "# normalize transcoding score\n",
    "mms = MinMaxScaler()\n",
    "knn_data['score'] = mms.fit_transform(np.array(knn_data['score']).reshape(-1,1))\n",
    "\n",
    "def convert_score(score):\n",
    "    if 0.0 <= score <= 0.3:\n",
    "        return 'Cheap'\n",
    "    if 0.3 <= score <= 0.7:\n",
    "        return 'Moderate'\n",
    "    if 0.7 <= score <= 1.0:\n",
    "        return 'Expensive'\n",
    "\n",
    "# compute the transcoding score to classification variable\n",
    "knn_data['score'] = knn_data['score'].apply(convert_score)\n",
    "\n",
    "knn_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the correlation scores between transcoding score and other variables\n",
    "knn_corr = knn_data.apply(lambda x : pd.factorize(x)[0]).corr(method='pearson', min_periods=1).round(3)\n",
    "pd.DataFrame(knn_corr['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the correlation scores\n",
    "plt.figure(figsize=(8,5))\n",
    "knn_corr['score'].drop(['score']).plot(kind='bar', width=0.6, color='deepskyblue')\n",
    "plt.axhline(y=0.0, color='black', linestyle='-')\n",
    "plt.axhline(y=0.3, color='salmon', linestyle='--')\n",
    "ax=plt.gca()\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ")\n",
    "plt.title('Correlation score for each feature')\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('correlation_score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "From the statistical and visualized correlation scores above, we can see that 'bitrate' has a correlation score that is higher than 0.3 with 'score', which indicates they are fairly correlated. Hence, I will use bitrate as predictor to predict transcoding score."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "My choice of Response and Predictors are\n",
    "\n",
    "Response: score\n",
    "Predictor: bitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)\n",
    "\n",
    "# split data\n",
    "train, test = train_test_split(knn_data, test_size=0.2, random_state=5)\n",
    "\n",
    "# scale data\n",
    "ss = StandardScaler()\n",
    "train.iloc[:,:-1] = ss.fit_transform(train.iloc[:,:-1])\n",
    "test.iloc[:,:-1] = ss.fit_transform(test.iloc[:,:-1])\n",
    "\n",
    "# set x, y for training and testing\n",
    "x_train, y_train, x_test, y_test = pd.DataFrame(train['bitrate']), pd.DataFrame(train['score']), pd.DataFrame(test['bitrate']), pd.DataFrame(test['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the optimal value of K in KNN\n",
    "\n",
    "acc = []\n",
    "\n",
    "for i in range(1,40):\n",
    "    neigh = KNeighborsClassifier(n_neighbors = i).fit(x_train,y_train)\n",
    "    yhat = neigh.predict(x_test)\n",
    "    acc.append(metrics.accuracy_score(y_test, yhat))\n",
    "    \n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,40),acc,color = 'royalblue',linestyle='dashed', \n",
    "         marker='o',markerfacecolor='coral', markersize=8)\n",
    "plt.title('Accuracy vs. K value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Accuracy')\n",
    "print(\"Maximum accuracy is\",max(acc).round(3),\"at K =\",acc.index(max(acc)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To determine the optimal K value in KNN, I visualize the plot between accuracy and K value. From the plot above, we can see that the maximum accuracy we got is 0.931 at K = 19, so we will get better efficiency at that K value.\n",
    "\n",
    "Let's use K=19 to build the KNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build KNN model and fit data\n",
    "knn = KNeighborsClassifier(n_neighbors=19)\n",
    "knn_model = knn.fit(x_train, y_train)\n",
    "\n",
    "# find the training and testing scores of the model \n",
    "print(\"Training Score:\", knn_model.score(x_train, y_train))\n",
    "print(\"Testing Score: \", knn_model.score(x_test, y_test))\n",
    "print(\"Overfitting:   \", knn_model.score(x_train, y_train)>knn_model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The Training Score and the Testing Score of the model are fairly high, which is nice.\n",
    "The Training Score is lower than the Testing Score, hence my KNN model avoids overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction of y_pred for x_test\n",
    "y_pred = knn_model.predict(x_test)\n",
    "\n",
    "# print normalized confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = (cm/cm.astype(np.float).sum(axis=1)).round(2)\n",
    "print(\"   C    M    E\")\n",
    "print(\"C\", cm[0][0], cm[0][1], \"\", cm[0][2])\n",
    "print(\"M\", cm[1][0], \"\", cm[1][1], \"\", cm[1][2])\n",
    "print(\"E\", cm[2][0], cm[2][1], \"\", cm[2][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "knn_matrix = pd.DataFrame(\n",
    "    {\"normalized_confusion_matrix\": [cm[0][0], cm[1][1], cm[2][2]]}, \n",
    "    index=['Cheap', 'Moderate', 'Expensive'])\n",
    "knn_matrix.plot(kind='bar', width=0.5, color='pink').get_legend().remove()\n",
    "ax=plt.gca()\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ")\n",
    "plt.title('Confusion matrix for each classification')\n",
    "plt.xlabel('score')\n",
    "plt.ylabel('confusion_matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The normalized confusion matrix shows that:\n",
    "our prediction of class Cheap has 97% accuracy,\n",
    "our prediction of class Expensive has 71% accuracy,\n",
    "our prediction of class Moderate has ZERO accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a text report showing the main classification metrics\n",
    "target_names = ['Cheap', 'Moderate', 'Expensive']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The classification report shows that:\n",
    "\n",
    "1. The overall accuracy score is high as 0.93, which seems nice, it is because of the high prediction accuracy of the class Cheap, however there is no accuracy for the prediction of the class Moderate.\n",
    "\n",
    "2. The precision score, recall score, and F1 score are all high for the class Cheap (about 0.97), and they are all moderate for the class Expensive (0.71), but the scores are all 0 for the class Moderate. \n",
    "\n",
    "3. The macro average metrics for precision score, recall score, and F1 score are all 0.56."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Enter your Response and Predictors here (for marker simplicity)\n",
    "\n",
    "Response: score\n",
    "Predictor: bitrate"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# YOUR RESPONSE HERE\n",
    "\n",
    "\n",
    "~Implementation process:\n",
    "\n",
    "1. I encode the categorical variables of the knn_data, which I inheirt from the previous question.\n",
    "\n",
    "2. I calculate the transcoding score by taking the average of umem and utime, and then normalizing it.\n",
    "\n",
    "3. I convert the transcoding score to different classfications: Cheap, Moderate, Expensive.\n",
    "\n",
    "4. I choose the transcoding score as the response. For the choice of predictors, I compute the correlation scores between the transcoding score and other variables, then I print the correlation score statistics and visualize them. I set the variable that has a correlation score more than 0.3 as the predictor (excluding umem and utime), which is bitrate.\n",
    "\n",
    "5. I split the dataset into training set (80%) and testing set (20%), and scale the two sets by StandardScaler, for a better fit of the KNN model. I also divide the training and testing set for the predictor and the response.\n",
    "\n",
    "6. I determine the optimal value of K in KNN by finding the maximum accuracy. The choice is K=19, my model will check 19 neighbors.\n",
    "\n",
    "7. I build the KNN model, fit the predictor training set and the response training set into it. To evaluate the model's performance, I calculate the Training Score and the Testing Score, and compare the two scores to detect overfitting.\n",
    "\n",
    "8. I get the response predicting set for the predictor testing set, and compare it with the response testing set to get the normalized confusion matrix which statistically indicates the prediction result. I also visualize the confusion matrix for different classfications. (See above for analysis)\n",
    "\n",
    "9. I generate a report to show some other classification scoring metrics, including: accuracy, precision, recall, and F1 score. (See above for analysis)\n",
    "\n",
    "\n",
    "~Strengths of the model:\n",
    "\n",
    "1. The model uses some rigorous methods to determine optimal classfication parameters. For instance, the choice of predictors is based on correlation matrix, the choice of K value in KNN is based on accuracy score.\n",
    "\n",
    "2. The model's attributes are pre-processed by normalization and scaling to avoid some interference factors.\n",
    "\n",
    "3. The model's performance is validated by taining/testing scores and various statistical classification metrics.\n",
    "\n",
    "4. The model avoids overfitting.\n",
    "\n",
    "\n",
    "~Pitifalls of the model:\n",
    "\n",
    "1. The biggest problem is that the model's accuracy scores for predicting different labels are significantly different.\n",
    "\n",
    "2. The issue derives from the instruction \"The transcoding score can be obtained by taking the average of umem and utime and then normalizing it.\" I observe that umem and utime are significantly different in numbers: umem is too large but utime is relatively small. Consequently, if I calculate the average of umem and utime, the result of transcoding score will be heavily skewed towards umem. After normalization, most of the classfication labels become 'Cheap'. Such an unbalance in distribution results in the high accuracy for predicting label 'Cheap', but 0 accuracy for predicting label 'Moderate'.\n",
    "\n",
    "\n",
    "~Future work:\n",
    "\n",
    "I will try constructing a second model which does normalization first and then computes the average. In this way, umem and utime's order of magnitude won't affect the KNN model fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "## Q3: Decision Trees for Digital Sky <span style= 'float: right;'><b>[20 marks]</b></span>\n",
    "\n",
    "The SDSS.csv data consists of 10,000 observations of space taken by the Sloan Digital Sky Survey, which offers public data of space observations. Every observation is described by 17 feature columns and 1 class column, which identifies it as either a star, galaxy or quasar.\n",
    "\n",
    "To ease your start with the data, you can read the feature descriptions in the [SDSS Description](./data/SDSS_Description.txt) file.\n",
    "\n",
    "**Your task is to implement a `Decision Tree Classification Algorithm` that can predict the `class` of an image**.\n",
    "\n",
    "You are required to perform the following tasks:\n",
    "\n",
    "1. Import the SDSS dataset and perform preprocessing as required.\n",
    "2. Create a new feature (or column) called `category` by converting the nominal categorical variable `class` into an ordinal categorical variable based on the table presented below:\n",
    "\n",
    "|  **class**      |  **category**        |\n",
    "|--------------------------|------------------------- |\n",
    "|  STAR                   |  0   |\n",
    "|  GALAXY                |  1 |\n",
    "|  QSO               |  2 |\n",
    "\n",
    "\n",
    "3. Implement an algorithm that can predict the `category` using the features present in the SDSS dataset.\n",
    "4. Perform independent testing of the model and provide statistical metrics outlining the performance of your model. Splitting the dataset into testing and training subsets will assist with this.\n",
    "5.  Plot the resulting Decision Tree produced by the Tree-Building algorithm.\n",
    "\n",
    "The QSO class refers to the quasi-stellar object, quasar. The order assigned to the `class` categorical variable is based on the brightness of the object, i.e. QSO is brighter than a GALAXY, and a GALAXY is brighter than a STAR.\n",
    "\n",
    "You are welcome to use any features within the dataset, except the `class` of the SDSS data. If required, you can also look to make new compound attributes that may help increase the accuracy of your model. You are expected to **verbally and visually** justify all aspects of your answer, including the features used, the metrics provided and the validation system employed. Provide commentary on the strengths and potential pitfalls of the model.\n",
    "\n",
    "<span style='color:red;'><b>Note:</b> You are only allowed to use packages that are within the Anaconda distribution. This means packages such as Keras, Tensorflow etc are not available for use.</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)\n",
    "\n",
    "# import data\n",
    "sdss_data = pd.read_csv(\"data/sdss.csv\")\n",
    "\n",
    "# drop NA if any\n",
    "sdss_data = sdss_data.dropna()\n",
    "\n",
    "def convert_class(object):\n",
    "    if object == 'STAR':\n",
    "        return '0'\n",
    "    if object == 'GALAXY':\n",
    "        return '1'\n",
    "    if object == 'QSO':\n",
    "        return '2'\n",
    "\n",
    "# encode class as ordinal categorical variable into category\n",
    "sdss_data['category'] = sdss_data['class'].apply(convert_class)\n",
    "\n",
    "# drop class column, rerun column(all values are same), and unnecessary columns (all kinds of id, serial number, date)\n",
    "sdss_data.drop(['class', 'rerun', 'camcol', 'objid', 'specobjid', 'mjd', 'fiberid'], axis = 1, inplace=True)\n",
    "\n",
    "sdss_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the correlation scores between category and other features\n",
    "sdss_corr = sdss_data.apply(lambda x : pd.factorize(x)[0]).corr(method='pearson', min_periods=1)\n",
    "pd.DataFrame(sdss_corr['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the correlation scores\n",
    "plt.figure(figsize=(8,5))\n",
    "sdss_corr['category'].drop(['category']).plot(kind='bar', width=0.6, color='salmon')\n",
    "plt.axhline(y=0.05, color='dodgerblue', linestyle='--')\n",
    "ax=plt.gca()\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ")\n",
    "plt.title('Correlation score for each feature')\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('correlation_score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features that have a correlation score higher than 0.05 with category, and keep the category column\n",
    "sdss_data = sdss_data[['run', 'field', 'redshift', 'plate', 'category']]\n",
    "sdss_data.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "My choice of Response and Predictors are\n",
    "\n",
    "Response: category\n",
    "Predictors: run, field, redshift, plate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)\n",
    "\n",
    "# split data\n",
    "train, test = train_test_split(sdss_data, test_size=0.2, random_state=0)\n",
    "\n",
    "# set x, y for training and testing\n",
    "x_train, y_train, x_test, y_test = train.iloc[:,:-1], train.iloc[:,-1], test.iloc[:,:-1], test.iloc[:,-1]\n",
    "\n",
    "# build GridSearchCV model\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "pipe = Pipeline(steps=[('dec_tree', clf)])\n",
    "parameters = dict(\n",
    "    dec_tree__max_depth=np.arange(1,11,2).tolist(),\n",
    "    dec_tree__min_samples_split=np.arange(2,11,2).tolist(),\n",
    "    dec_tree__min_samples_leaf=np.arange(2,11,2).tolist()\n",
    ")\n",
    "clf_GS = GridSearchCV(pipe, parameters, n_jobs=-1, cv=2)\n",
    "clf_GS.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot grid scores from GridSearchCV\n",
    "def plot_search_results(grid):\n",
    "    ## Results from grid search\n",
    "    results = grid.cv_results_\n",
    "    means_test = results['mean_test_score']\n",
    "    stds_test = results['std_test_score']\n",
    "\n",
    "    ## Getting indexes of values per hyper-parameter\n",
    "    masks=[]\n",
    "    masks_names= list(grid.best_params_.keys())\n",
    "    for p_k, p_v in grid.best_params_.items():\n",
    "        masks.append(list(results['param_'+p_k].data==p_v))\n",
    "\n",
    "    params=grid.param_grid\n",
    "\n",
    "    ## Ploting results\n",
    "    fig, ax = plt.subplots(1,len(params),sharex='none', sharey='all',figsize=(15,5))\n",
    "    fig.suptitle('Score for each hyper-parameter')\n",
    "    fig.text(0.04, 0.5, 'MEAN SCORE', va='center', rotation='vertical')\n",
    "    pram_preformace_in_best = {}\n",
    "    for i, p in enumerate(masks_names):\n",
    "        m = np.stack(masks[:i] + masks[i+1:])\n",
    "        pram_preformace_in_best\n",
    "        best_parms_mask = m.all(axis=0)\n",
    "        best_index = np.where(best_parms_mask)[0]\n",
    "        x = np.array(params[p])\n",
    "        y_1 = np.array(means_test[best_index])\n",
    "        e_1 = np.array(stds_test[best_index])\n",
    "        ax[i].errorbar(x, y_1, e_1, label='test', linestyle='--', marker='o', markersize=10, linewidth=3, color='dodgerblue', markerfacecolor='tomato')\n",
    "        ax[i].set_xlabel(p.upper())\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_search_results(clf_GS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get optimal hyper parameters of a DecisionTree\n",
    "max_depth = clf_GS.best_estimator_.get_params()['dec_tree__max_depth']\n",
    "min_samples_split = clf_GS.best_estimator_.get_params()['dec_tree__min_samples_split']\n",
    "min_samples_leaf = clf_GS.best_estimator_.get_params()['dec_tree__min_samples_leaf']\n",
    "\n",
    "print('Best max_depth:', max_depth)\n",
    "print('Best min_samples_split:', min_samples_split)\n",
    "print('Best min_samples_leaf:', min_samples_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the optimal parameters to build the DecisionTree model and fit the training dataset\n",
    "clf = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, random_state=0)\n",
    "clf = clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation scores\n",
    "print(\"Training Score:\", clf.score(x_train, y_train).round(2))\n",
    "print(\"Testing Score: \", clf.score(x_test, y_test).round(2))\n",
    "print(\"Overfitting:   \", clf.score(x_train, y_train).round(2)>clf.score(x_test, y_test).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction of y_pred for x_test\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "# print normalized confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = (cm/cm.astype(np.float).sum(axis=1)).round(2)\n",
    "print(\"   S    G    Q\")\n",
    "print(\"S\", cm[0][0], cm[0][1], \"\", cm[0][2])\n",
    "print(\"G\", cm[1][0], cm[1][1], cm[1][2])\n",
    "print(\"Q\", cm[2][0], cm[2][1], cm[2][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "knn_matrix = pd.DataFrame(\n",
    "    {\"normalized_confusion_matrix\": [cm[0][0], cm[1][1], cm[2][2]]}, \n",
    "    index=['STAR', 'GALAXY', 'QSO'])\n",
    "knn_matrix.plot(kind='bar', width=0.5, color='deepskyblue').get_legend().remove()\n",
    "ax=plt.gca()\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ")\n",
    "plt.title('Confusion matrix for each class')\n",
    "plt.xlabel('class')\n",
    "plt.ylabel('confusion_matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a text report showing the main classification metrics\n",
    "target_names = ['STAR', 'GALAXY', 'QSO']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the decision tree\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "\n",
    "tree_plot = tree.plot_tree(clf, filled=True)\n",
    "\n",
    "for o in tree_plot:\n",
    "    arrow = o.arrow_patch\n",
    "    if arrow is not None:\n",
    "        arrow.set_edgecolor('grey')\n",
    "        arrow.set_linewidth(3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Enter your Response and Predictors here (for marker simplicity)\n",
    "\n",
    "Response: category\n",
    "Predictors: run, field, redshift, plate"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# YOUR RESPONSE HERE\n",
    "\n",
    "\n",
    "~Implementation process:\n",
    "\n",
    "1. I import and pre-process the sdss dataset as required.\n",
    "\n",
    "2. I use Pearson correlation to compute the correlation scores between the category and other variables, then I print the correlation score statistics and visualize them. Based on the result, I extract features that have a relatively high correlation score with category as predictors, and category as response, to build the DecisionTree model.\n",
    "\n",
    "3. I split the dataset into training set (80%) and testing set (20%), then divide the training and testing set for the predictor and the response.\n",
    "\n",
    "4. I build GridSearchCV model, visualize the mean scores for hyper-parameters: max_depth, min_samples_split, and min_samples_leaf of the DecisionTree, and print the optimal values for them.\n",
    "\n",
    "5. I use these optimal parameters to build the DecisionTree model and fit the training dataset.\n",
    "\n",
    "6. To evaluate the model's performance, I calculate the Training Score and the Testing Score, and compare the two scores to detect overfitting.\n",
    "\n",
    "7. I get the response predicting set for the predictor testing set, and compare it with the response testing set to get the normalized confusion matrix which statistically indicates the prediction result. I also visualize the confusion matrix for different classfications.\n",
    "\n",
    "8. I generate a report to show some other classification scoring metrics, including: accuracy, precision, recall, and F1 score.\n",
    "\n",
    "9. I plot a figure of the decision tree model.\n",
    "\n",
    "\n",
    "~ How does my decision tree work:\n",
    "\n",
    "From the plot above, I observe that my decision tree makes splits only according to the 'redshift' feature. Starting from the split of the root node, the model keeps measuring the subsets' impurity by the gini score and perform the best split according to the one that produces the purest subsets (Ceballos, 2019). The max_depth, min_samples_split, and min_samples_leaf are the parameters that constrain the expand of the notes, so that the model avoids overfitting.\n",
    "\n",
    "\n",
    "~ Validity of the model:\n",
    "\n",
    "1. The Training Score and the Testing Score of the model are high as 0.99, which is nice.\n",
    "\n",
    "2. The Training Score does not exceed the Testing Score, hence my decision tree model avoids overfitting.\n",
    "\n",
    "3. The normalized confusion matrix shows that our prediction of class 'STAR', 'GALAXY', 'QSO' have accruacy of 100%, 99%, 92%. That's a success!\n",
    "\n",
    "4. The classification report shows that the model's overall accuracy score is 0.99, and the macro average metrics for precision score, recall score, F1 score are 0.98, 0.97, 0.98. Three classes' classfication scoring metrics all exceed 0.90. Hence, my decision tree model satisfies the validity.\n",
    "\n",
    "\n",
    "~Strengths of the model:\n",
    "\n",
    "1. The model uses correlation score to determine features used as predictors.\n",
    "\n",
    "1. The model uses GridSearchCV to determine optimal hyper-parameters for the DecisionTree.\n",
    "\n",
    "3. The model's performance can be validated by taining/testing scores and various statistical classification metrics.\n",
    "\n",
    "\n",
    "~Pitifalls of the model:\n",
    "\n",
    "1. During the selection of predictors, I find that none of the other features have a correlation score higher than 0.3 with the 'category' feature. This may cause potential overfitting for my decision tree, because they are not so relevant with the response feature.\n",
    "\n",
    "1. As observed from the decision tree plot, the splitting questions in the tree are all based on the 'redshift' feature, which is not normal for a typical decision tree. I think it is because I do not optimize 'max_features' during hyper-parameter optimization, thus this parameter is constrained by other parameters like 'max_depth'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "## Q4: Serious SQL <span style= 'float: right;'><b>[20 marks]</b></span>\n",
    "Consider the following scenario.\n",
    "\n",
    "> You are applying for a job as a database developer for an unnamed wrestling company. Part of the job description includes creating an automation system for running SQL queries. During the hiring process, the interviewers want to ensure you understand the SQL language. They have provided a set of questions to be answered by you, and your responses will later be reviewed by them. They are unwilling to give you access to their real database (which is mysteriously missing), so they have provided an SQLite3 database and asked you to interact with it using Python. \n",
    "\n",
    "\n",
    "Based on the above scenario, you have been asked to answer a number of questions to test your skills. You will be using the Northwind database for this question. The database model is as follows:\n",
    "\n",
    "![Northwind_Database](./img/er_diagram.png)\n",
    "\n",
    "In the following questions, you will be asked to execute the SQL statement, and explain any reasoning as necessary. Data can be formatted as raw printed output or a Pandas DataFrame. Recall the use of the `fetchone` and `fetchall` functions on an sqlite cursor for retriving information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS YOUR CONNECTION BLOCK, DO NOT MODIFY THIS. \n",
    "# OTHERWISE, YOU WILL NOT BE ABLE TO READ THE DATABASE\n",
    "def create_connection():\n",
    "    \"\"\" create a database connection to a database that resides\n",
    "        in the memory\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(':memory:')\n",
    "        print(\"Connection established!\")\n",
    "        \n",
    "    except Error as e:\n",
    "        print(\"Error Connecting to Database\")\n",
    "        raise(e)\n",
    "    return conn\n",
    "\n",
    "northwind_sql = 'data/northwind.sql'\n",
    "conn = create_connection() \n",
    "cur = conn.cursor()\n",
    "qry = open(northwind_sql, 'r').read()\n",
    "cur.executescript(qry)\n",
    "conn.commit()\n",
    "# remember to close the connection when everything is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql(query):\n",
    "    # Select table and display\n",
    "    cur.execute(query)\n",
    "\n",
    "    # Fetches all the rows from the result of the query\n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    # Gets the column names for the table\n",
    "    colnames = [desc[0] for desc in cur.description]\n",
    "\n",
    "    # Converts into readable pandas dataframe\n",
    "    df_result = pd.DataFrame(rows, columns=colnames)\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: Customised Customers <span style= 'float: right;'><b>[3 marks]</b></span>\n",
    "\n",
    "Retrieve the details of all the customers whose `ContactTitle` is Owner or is located in the `Country` Mexico. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)\n",
    "\n",
    "execute_sql(\n",
    "    \"\"\"\n",
    "    \n",
    "    select * from Customers where ContactTitle = 'Owner' or Country = 'Mexico';\n",
    "\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Customers With No Orders <span style= 'float: right;'><b>[3 marks]</b></span>\n",
    "There are some customers who have never actually placed an order. Show these customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)\n",
    "\n",
    "execute_sql(\n",
    "    \"\"\"\n",
    "    \n",
    "    select * from Customers where CustomerID not in (select CustomerID from Orders);\n",
    "    \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3: Products & their Categories <span style= 'float: right;'><b>[3 marks]</b></span>\n",
    "\n",
    "Count the total quantity sold for each products. Retrieve the `ProductID`, `ProductName`, `CategoryName`, and the total quantity sold as `Total Sold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)\n",
    "\n",
    "execute_sql(\n",
    "    \"\"\"\n",
    "    \n",
    "    select Products.ProductID, Products.ProductName, Categories.CategoryName, sum('Order Details'.Quantity) as 'Total Sold'\n",
    "    from 'Order Details' \n",
    "    left join Products on 'Order Details'.ProductID = Products.ProductID \n",
    "    left join Categories on Products.CategoryID = Categories.CategoryID \n",
    "    group by Products.ProductID;\n",
    "    \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4: Products Above Average Price <span style= 'float: right;'><b>[3 marks]</b></span>\n",
    "\n",
    "Retrieve the `ProductName` and `UnitPrice` of all the products whose price is greater than the average price of all the products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)\n",
    "\n",
    "execute_sql(\n",
    "    \"\"\"\n",
    "    \n",
    "    select ProductName, UnitPrice from Products where UnitPrice > (select avg(UnitPrice) from Products);\n",
    "\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5: Most Expensive Products <span style= 'float: right;'><b>[3 marks]</b></span>\n",
    "\n",
    "Get the `ProductName` and the `UnitPrice` of the top 10 most expensive products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)\n",
    "\n",
    "execute_sql(\n",
    "    \"\"\"\n",
    "    \n",
    "    select ProductName, UnitPrice from Products order by UnitPrice desc limit 10;\n",
    "\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6: Stocktaking Products by Category and Supplier <span style= 'float: right;'><b>[5 marks]</b></span>\n",
    "\n",
    "Get the total number of units that are in stock for each `ProductCategory` for each `Supplier Continent`. The resulting table should contain three columns: `ProductCategory`, `Supplier Continent`, and `UnitsInStock`.\n",
    "\n",
    "`Supplier Continent` can be obtained by mapping the values present in `Country` to their relevant continent. The table below contains the list of countries and the continent that they belong to.\n",
    "\n",
    "|  **Country**      |  **Supplier Continent**        |\n",
    "|--------------------------|------------------------- |\n",
    "|  UK, Spain, Sweden, Germany, Norway, Denmark, Netherlands, Finland, Italy, France                   |  Europe   |\n",
    "|  USA, Canada, Brazil                |  America |\n",
    "|  Australia, Japan, Singapore               |  Asia-Pacific |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (ANY ADDITIONAL CELLS AS REQUIRED)\n",
    "\n",
    "\n",
    "execute_sql(\n",
    "    \"\"\"\n",
    "\n",
    "    select Categories.CategoryName as ProductCategory, \n",
    "    case \n",
    "         when Suppliers.Country in ('UK', 'Spain', 'Sweden', 'Germany', 'Norway', 'Denmark', 'Netherlands', 'Finland', 'Italy', 'France')\n",
    "         then 'Europe'\n",
    "         when Suppliers.Country in('USA', 'Canada', 'Brazil')\n",
    "         then 'America'\n",
    "         else 'Asia-Pacific'\n",
    "    end as 'Supplier Continent',\n",
    "    sum(Products.UnitsInStock) as UnitsInStock\n",
    "    from Products \n",
    "    left join Categories on Products.CategoryID = Categories.CategoryID\n",
    "    left join Suppliers on Products.SupplierID = Suppliers.SupplierID\n",
    "    group by ProductCategory, \n",
    "    case \n",
    "         when Suppliers.Country in ('UK', 'Spain', 'Sweden', 'Germany', 'Norway', 'Denmark', 'Netherlands', 'Finland', 'Italy', 'France')\n",
    "         then 'Europe'\n",
    "         when Suppliers.Country in('USA', 'Canada', 'Brazil')\n",
    "         then 'America'\n",
    "         else 'Asia-Pacific'\n",
    "    end;\n",
    "\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "## Q5: Ethics and Security <span style= 'float: right;'><b>[15 marks]</b></span>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1: OkCupid Data Scrape <span style= 'float: right;'><b>[5 marks]</b></span>\n",
    "\n",
    "While Data Scientists and Data Engineers spend a lot of timing thinking about how to solve a problem, it is important to think about _why_ we solve a problem and what impacts it could have. For the following scenario, provide a written response to the questions.\n",
    "\n",
    "> In 2016, almost 70,000 Okcupid profiles had their data released onto the Open Science Framework. This place is an online community where people share raw data and collaborate with each other over data sets. Two Danish researchers, Emil Kirkegaard and Julius Daugbjerg-Bjerrekaer, scraped the data with a bot profile on Okcupid and released publicly identifiable information such as age, gender, sexual orientation, and personal responses to the survey questions the website asks when people sign up for a profile. More importantly, the two researchers didnt feel their actions were explicitly or ethically wrong, because Data is already public. This huge data release raised eyebrows and forced questions about the ethics of releasing already public data. \n",
    "\n",
    "What does big data ethics have to say about already public data? What harms could arise from the outcomes of the two Danish researchers' actions?\n",
    "\n",
    "Provide examples in your response to the questions.\n",
    "\n",
    "**NOTE:** Marks will be awarded based on the brevity and clarity of the arguments and not on quantity. Do not exceed more than 300 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Response\n",
    "*****\n",
    "\n",
    "**Ethics**\n",
    "\n",
    "When it comes to releasing \"already public\" data, many basic requirements of research ethics cannot sufficiently addressed in this scenario. The data is already public is an excuse to cover thorny ethical concerns. It is because public does not equally consent to this act. There must be guidelines. For instance, Western Australia government suggests that: \"*Data that is released must not be (or be able to be) associated with any individual. Before releasing data, agencies should carefully consider and address any privacy concerns.*\"\n",
    "\n",
    "**Harms**\n",
    "\n",
    "Even if someone is free to share a piece of information, the big data is able to amplify it in a way that the person never intended or agreed. For instance, in 2008, some researchers publicly released profile data collected from the Facebook accounts of students at a US university, although they tried to conceal the university's identity and protect the students' privacy, the source of the data was easily revealed, putting the students' privacy at risk.\n",
    "\n",
    "Besides, such leak of data may leads to discrimination and bully against specific communites, for instance, discrimination against sexual orientation.\n",
    "\n",
    "#### Reference\n",
    "\n",
    ">- *(Zimmer, 2010)*\n",
    ">- *(Zimmer, 2016)*\n",
    ">- *(Guide and checklist to identifying and preparing data for release, 2021)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2: Digital Signature <span style= 'float: right;'><b>[8 marks]</b></span>\n",
    "\n",
    "Consider the following scenario:\n",
    "\n",
    "> Ray Technologies has outsourced some work to Lux and Kay, and needs them to jointly sign a contract, which it will then also sign. Since all of the parties involved are located in different parts of the world, and this is a frequently occurring scenario, Ray Technologies decides to come up with a method for doing this electronically. The contract has to be signed by both Lux and Kay, and then finally by Ray Technologies. We assume that the contract is transmitted electronically over public channels, so integrity and confidentiality have to be assured. Both Lux and Kay need to be assured that they are both signing the same contract and need to each have a copy of the contract signed by all three parties involved. The contract needs to be non-repudiable and the process has to be efficient.\n",
    "\n",
    "Describe a method that Ray Technologies can use for this purpose that uses cryptographic techniques and meets the above requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Response\n",
    "*****\n",
    "\n",
    "To assure the contract is confidential and integral, the method should be mixing RSA and ASE.\n",
    "\n",
    "1.  Ray technology informs Lux to send the request.\n",
    "2.  Lux generates RSA_PUBLIC_KEY and RSA_PRIVATE_KEY, and then randomly generates ASE_KEY.\n",
    "3.  Lux sends RSA_PUBLIC_KEY to Ray Technologies, and keeps RSA_PUBLIC_KEY.\n",
    "4.  Lux uses RSA_PRIVATE_KEY asymmetric encryption AES_key and ENCRYPT_ASE_KEY.\n",
    "5.  Lux sends ENCRYPT_ASE_KEY to Ray Technologies, and uses RSA_PUBLIC_KEY to decrypt ENCRYPT_ASE_KEY.\n",
    "6.  Ray Technologies uses ASE_KEY to symmetrically encrypt the contract to be encrypted and generates encrypted data ENCRYPTED_CONTRACT.\n",
    "7.  Ray Technologies sends ENCRYPTED_CONTRACT to Lux, and uses ASE_KEY to decrypt and sign ENCRYPTED_CONTRACT.\n",
    "8.  Lux uses ASE_KEY to encrypt the signed contract to get ENCRYPTED_LUX_CONTRACT to connect to Ray Technologies.\n",
    "9.  Ray Technologies uses ASE_KEY to decrypt ENCRYPTED_LUX_CONTRACT and informs Kay to send the request.\n",
    "10. Kay generates RSA_PUBLIC_KEY_NEW and RSA_PRIVATE_KEY_NEW, and then randomly generates ASE_KEY_NEW.\n",
    "11. Kay sends RSA_PUBLIC_KEY_NEW to Ray Technologies and keeps PUBLIC_KEY_NEW.\n",
    "12. Kay uses RSA_PRIVATE_KEY_NEW asymmetric encryption AES_key_new and ENCRYPT_ASE_KEY_NEW.\n",
    "13. Kay sends ENCRYPT_ASE_KEY_NEW to Ray Technologies, and uses RSA_PUBLIC_KEY_NEW to decrypt ENCRYPT_ASE_KEY_NEW to get ASE_KEY_NEW.\n",
    "14. Ray Technologies uses ASE_KEY_NEW to encrypt Luxs signed contract to generate encrypted data ENCRYTED_CONTRT_NEW.\n",
    "15. Ray Technologies sends ENCRYPTED_CONTRACT_NEW to Kay, and uses ASE_KEY_NEW to decrypt and sign ENCRYPTED_CONTRACT_NEW.\n",
    "16. Kay uses ASE_KEY_NEW to encrypt the signed contract to get the BOTH_SIGNED_CONTRACT connection and send it back to Ray technology.\n",
    "17. Ray Technologies used ASE_KEY_NEW to decrypt BOTH_SIGNED_CONTRACT to obtain the contract they signed together.\n",
    "18. Ray Technologies uses ASE_KEY and ASE_KEY_NEW to encrypt the signed contract, and then sends it to Lux and Kay. You can use ASE_KEY and ASE_KEY_NEW to save a copy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3: Resilient to the future <span style= 'float: right;'><b>[2 marks]</b></span>\n",
    "\n",
    "As technology evolves, so does the need for secure cryptographic algorithms. With the introduction of quantum computing, the organisations around the world are preparing to migrate towards quantum resistant algorithms.\n",
    "\n",
    "Briefly explain why there is a need for quantum-resistant cryptographic algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Response\n",
    "*****\n",
    "\n",
    "Quantum computing is known for its speed advantage so that it can perform new types of computation that classical computers can not do.\n",
    "\n",
    "Quantum computing can **break cryptography**, hence it can deeply harm IT security.\n",
    "\n",
    "In terms of **public-key cryptography**, powerful quantum computers will threaten the public-key cryptography systems because they can carry out the decryption without knowing the private key priorly. The mainstream public-key cryptography algorithms, including Rivest-Shamir-Adleman (RSA), Elliptic Curve Digital Signature Algorithm (ECDSA), Digital Signature Algorithm (DSA), and Diffie-Hellman key agreement protocol, cannot resist quantum computers.\n",
    "\n",
    "The effected instances can be:\n",
    "\n",
    "   - digital signatures\n",
    "   - Internet protocols\n",
    "   - online banking\n",
    "   - online shopping\n",
    "\n",
    "In terms of **symmetric cryptography**, quantum computing will threaten symmetric cryptography systems like Advanced Encryption Standard (AES). AES, as symmetric cryptography, needs to exchange private keys securely and confidentially. However, the key exchange methods today are based on mathematical problems that may be solved by powerful quantum algorithms (Shor's algorithm, Grover's algorithm, etc.), hence symmetric cryptography is also at risk.\n",
    "\n",
    "**Quantum-resistant cryptography**, or post-quantum cryptography, is cryptography whose security is theoretically unaffected by quantum computing. It utilizes different math building blocks and operations that quantum computers have no advantage over other classical computers.\n",
    "\n",
    "With the emergence of quantum computers nowadays, quantum-resistant cryptography is getting more important for us to protect **data security** and **confidentiality of communications**, even from the threats of the future.\n",
    "\n",
    "#### Reference\n",
    "\n",
    "> *(Olejnik, Riemann and Zerdick, 2020)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
